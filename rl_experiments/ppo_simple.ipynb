{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Simple PPO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will implement a very simple single-threaded version of PPO using only one actor and then attempt to solve the CartPole-v1 task."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Critic\n",
    "The critic will approximate the value function by training a neural net using the TD(λ) algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 770,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.nn.init as init\n",
    "\n",
    "class CriticNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        # CartPole-v1 observation is a 4-dimensional vector of floats\n",
    "        # self.fc1 = nn.Linear(4, 10)\n",
    "        # We want to output a scalar state value\n",
    "        # self.fc2 = nn.Linear(10, 1)\n",
    "        self.fc1 = nn.Linear(4, 1)\n",
    "        init.zeros_(self.fc1.weight)\n",
    "        init.zeros_(self.fc1.bias)\n",
    "        \n",
    "        # initialize weights\n",
    "        #for layer in [self.fc1, self.fc2]:\n",
    "        #    init.xavier_uniform_(layer.weight)\n",
    "        #    init.zeros_(layer.bias)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        #x = F.relu(self.fc1(x))\n",
    "        #x = F.relu(self.fc2(x))\n",
    "        x = self.fc1(x)\n",
    "        return x\n",
    "\n",
    "class Critic:\n",
    "    \"\"\"Approximate value function with TD(λ).\n",
    "    \n",
    "    Arguments:\n",
    "    learning_rate — corresponds to α in Sutton\n",
    "    trace_decay_rate — corresponds to λ in Sutton\n",
    "    \"\"\"\n",
    "    def __init__(self, learning_rate, trace_decay_rate):\n",
    "        self.learning_rate = learning_rate\n",
    "        self.trace_decay_rate = trace_decay_rate\n",
    "        self.critic_net = CriticNet()\n",
    "        self.eligibility_trace = [\n",
    "            torch.zeros_like(parameter.data)\n",
    "            for parameter in self.critic_net.parameters()\n",
    "        ]\n",
    "    \n",
    "    def step(self, previous_state, state, reward):\n",
    "        \"\"\"Compute TD error and perform one step of optimization of critic_net.\n",
    "        \n",
    "        Arguments:\n",
    "        previous_state — corresponds to S_t in Sutton.\n",
    "        state — corresponds to S_{t+1} in Sutton.\n",
    "        reward — corresponds to R_{t+1} in Sutton.\n",
    "        \n",
    "        Return approximate value of previous_state, approximate value of state.\n",
    "        \"\"\"\n",
    "        self.critic_net.zero_grad()\n",
    "        previous_state_value = self.critic_net(previous_state)\n",
    "        previous_state_value.backward()\n",
    "        state_value = self.critic_net(state)\n",
    "        \n",
    "        #for i, parameter in enumerate(self.critic_net.parameters()):\n",
    "        #    self.eligibility_trace[i] *= self.trace_decay_rate\n",
    "        #    self.eligibility_trace[i] += parameter.grad.data\n",
    "        \n",
    "        td_error = reward + state_value.item() - previous_state_value.item()\n",
    "        tde.append(td_error)\n",
    "        for i, parameter in enumerate(self.critic_net.parameters()):\n",
    "            #parameter.data += self.learning_rate * td_error * self.eligibility_trace[i]\n",
    "            parameter.data += self.learning_rate * td_error * parameter.grad.data\n",
    "        \n",
    "        return previous_state_value.item(), state_value.item()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Actor\n",
    "The actor will record a trajectory of a fixed size and estimate advantages via GAE."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 616,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Actor:\n",
    "    def __init__(self, env, critic, trajectory_length, gae_gamma, gae_lambda):\n",
    "        \"\"\"Record trajectory of fixed length and estimate advantages via (truncated) GAE.\n",
    "        \n",
    "        Arguments:\n",
    "        env — environment implementing OpenAI gym interface.\n",
    "        critic — reference to critic, as defined above\n",
    "        trajectory_length — length of trajectory to record from env at each step.\n",
    "        gae_gamma — γ parameter from GAE paper.\n",
    "        gae_lambda — λ parameter from GAE paper.\n",
    "        \"\"\"\n",
    "        self.env = env\n",
    "        self.critic = critic\n",
    "        self.trajectory_length = trajectory_length\n",
    "        self.gae_gamma = gae_gamma\n",
    "        self.gae_lambda = gae_lambda\n",
    "        self.state = None\n",
    "        self.done = True\n",
    "    \n",
    "    def step(self, policy):\n",
    "        \"\"\"Perform single step, recording trajectory of fixed length and estimating advantages.\n",
    "        \n",
    "        Arguments:\n",
    "        policy — current policy, implementing sample_action(state) that chooses\n",
    "        the next action when in a given state according to the policy.\n",
    "        \n",
    "        Return the states encountered, the actions encountered, and the computed advantages.\n",
    "        \"\"\"\n",
    "        trajectory_states = torch.zeros(self.trajectory_length, 4)\n",
    "        trajectory_actions = torch.zeros(self.trajectory_length)\n",
    "        td_errors = torch.zeros(self.trajectory_length)\n",
    "        \n",
    "        for t in range(self.trajectory_length):\n",
    "            if self.done:\n",
    "                self.state = torch.tensor(self.env.reset(), dtype=torch.float32)\n",
    "                \n",
    "            trajectory_states[t] = self.state\n",
    "            action = policy.sample_action(self.state)\n",
    "            assert action in [0, 1]\n",
    "            trajectory_actions[t] = action\n",
    "            previous_state = self.state\n",
    "            self.state, reward, done, _ = env.step(action)\n",
    "            self.state = torch.tensor(self.state, dtype=torch.float32)\n",
    "            previous_state_value, state_value = self.critic.step(previous_state, self.state, reward)\n",
    "            td_errors[t] = reward + self.gae_gamma * state_value - previous_state_value\n",
    "        \n",
    "        #advantages = torch.zeros(self.trajectory_length)\n",
    "        #previous_advantage = 0\n",
    "        #\n",
    "        #for t in range(self.trajectory_length - 1, -1, -1):\n",
    "        #    advantage = self.gae_gamma * self.gae_lambda * previous_advantage + td_errors[t]\n",
    "        #    advantages[t] = advantage\n",
    "        #    previous_advantage = advantage\n",
    "        advantages = td_errors\n",
    "        \n",
    "        return trajectory_states, trajectory_actions, advantages"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ## Policy\n",
    " We will use a parametric policy that computes preference scores via a neural network and computes probabilities as the softmax of the preference scores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 617,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "class PolicyNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__() \n",
    "        # CartPole-v1 observation is a 4-dimensional vector of floats\n",
    "        self.fc1 = nn.Linear(4, 1000)\n",
    "        # We want to output a preferences score for each of the two actions.\n",
    "        self.fc2 = nn.Linear(1000, 2)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        return x\n",
    "\n",
    "def softmax(x):\n",
    "    \"\"\"Compute softmax vector of the vector x.\n",
    "    \n",
    "    This function supports vectorization. That is,\n",
    "    softmax(torch.stack([t1, t2, ...])) is equal to\n",
    "    torch.tensor([softmax(t1), softmax(t2), ...])\n",
    "    \"\"\"\n",
    "    exps = torch.exp(x)\n",
    "    if x.dim() == 1:\n",
    "        return exps / torch.sum(exps)\n",
    "    elif x.dim() == 2:\n",
    "        return torch.true_divide(exps, torch.sum(exps, dim=1).reshape(-1, 1))\n",
    "\n",
    "class Policy:\n",
    "    def __init__(self):\n",
    "        self.policy_net = PolicyNet()\n",
    "    \n",
    "    def sample_action(self, state):\n",
    "        preference_scores = self.policy_net(state)\n",
    "        probabilities = softmax(preference_scores)\n",
    "        # action for CartPole-v1 is either 0 or 1\n",
    "        action = random.choices(\n",
    "            [0, 1],\n",
    "            weights=probabilities\n",
    "        )[0]\n",
    "        return action\n",
    "    \n",
    "    def probability(self, state, action):\n",
    "        preference_scores = self.policy_net(state)\n",
    "        probabilities = softmax(preference_scores)\n",
    "        \n",
    "        if state.dim() == 1:\n",
    "            return probabilities[action]\n",
    "        elif state.dim() == 2:\n",
    "            return torch.gather(probabilities, 1, action.reshape(-1, 1).long()).flatten()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PPO\n",
    "Finally, we actually implement the PPO algorithm. We follow the \"actor-critic style\" pseudocode given in the PPO paper, using only a single thread and setting $N=1$ (using only one actor)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 618,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PPO:\n",
    "    def __init__(self, actor, policy, learning_rate, epsilon, K):\n",
    "        self.actor = actor\n",
    "        self.policy = policy\n",
    "        self.learning_rate = learning_rate\n",
    "        self.epsilon = epsilon\n",
    "        self.K = K\n",
    "    \n",
    "    def step(self):\n",
    "        trajectory_states, trajectory_actions, advantages = self.actor.step(self.policy)\n",
    "        advantages = advantages.detach()\n",
    "        old_probability = policy.probability(trajectory_states, trajectory_actions).detach()\n",
    "        print(f'adv = {advantages}')\n",
    "        \n",
    "        first_objective = None\n",
    "        last_objective = None\n",
    "        \n",
    "        for epoch in range(self.K):\n",
    "            probability = policy.probability(trajectory_states, trajectory_actions)\n",
    "            ratio = probability / old_probability\n",
    "            objective = torch.mean(\n",
    "                torch.min(torch.clamp(ratio, 1 - self.epsilon, 1 + self.epsilon) * advantages, ratio * advantages)\n",
    "            )\n",
    "            \n",
    "            if epoch == 0:\n",
    "                first_objective = objective.item()\n",
    "            elif epoch == self.K - 1:\n",
    "                last_objective = objective.item()\n",
    "            \n",
    "            policy.policy_net.zero_grad()\n",
    "            objective.backward()\n",
    "        \n",
    "            for parameter in policy.policy_net.parameters():\n",
    "                parameter.data += self.learning_rate * parameter.grad.data\n",
    "        \n",
    "        return first_objective, last_objective"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 619,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_policy_episode(env, policy):\n",
    "    state = env.reset()\n",
    "    score = 0\n",
    "    \n",
    "    while True:\n",
    "        action = policy.sample_action(torch.tensor(state, dtype=torch.float32))\n",
    "        state, reward, done, _ = env.step(action)\n",
    "        score += reward\n",
    "        \n",
    "        if done:\n",
    "            break\n",
    "            \n",
    "    return score\n",
    "\n",
    "def evaluate_policy(env, policy, num_episodes=10):\n",
    "    total_score = 0\n",
    "    \n",
    "    for _ in range(num_episodes):\n",
    "        total_score += evaluate_policy_episode(env, policy)\n",
    "    \n",
    "    return total_score / num_episodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 620,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "\n",
    "env = gym.make('CartPole-v1')\n",
    "policy = Policy()\n",
    "critic = Critic(0.01, 0.8)\n",
    "actor = Actor(env, critic, 100, 0.96, 0.98)\n",
    "ppo = PPO(actor, policy, 0.01, 0.2, 10)\n",
    "perf = []\n",
    "first = []\n",
    "last = []\n",
    "tde = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 652,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "adv = tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1.])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(1.0, 1.0)"
      ]
     },
     "execution_count": 652,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ppo.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 653,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " ...]"
      ]
     },
     "execution_count": 653,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tde"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 665,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.0000],\n",
       "        [0.0000],\n",
       "        [0.0681],\n",
       "        [0.0000],\n",
       "        [0.0000],\n",
       "        [0.0000],\n",
       "        [0.0000],\n",
       "        [0.0000],\n",
       "        [0.0000],\n",
       "        [0.0904],\n",
       "        [0.0000],\n",
       "        [0.0000],\n",
       "        [0.0000],\n",
       "        [0.0000],\n",
       "        [0.0919],\n",
       "        [0.0000],\n",
       "        [0.0000],\n",
       "        [0.0000],\n",
       "        [0.0000],\n",
       "        [0.0000],\n",
       "        [0.0000],\n",
       "        [0.0000],\n",
       "        [0.1199],\n",
       "        [0.0000],\n",
       "        [0.0000],\n",
       "        [0.0821],\n",
       "        [0.0000],\n",
       "        [0.0000],\n",
       "        [0.0000],\n",
       "        [0.0000],\n",
       "        [0.0000],\n",
       "        [0.0000],\n",
       "        [0.0000],\n",
       "        [0.0000],\n",
       "        [0.0000],\n",
       "        [0.0000],\n",
       "        [0.0000],\n",
       "        [0.0000],\n",
       "        [0.0000],\n",
       "        [0.1590],\n",
       "        [0.0000],\n",
       "        [0.0000],\n",
       "        [0.3560],\n",
       "        [0.0000],\n",
       "        [0.0584],\n",
       "        [0.0000],\n",
       "        [0.0000],\n",
       "        [0.3035],\n",
       "        [0.0000],\n",
       "        [0.0233],\n",
       "        [0.0000],\n",
       "        [0.0647],\n",
       "        [0.0000],\n",
       "        [0.1458],\n",
       "        [0.0000],\n",
       "        [0.0000],\n",
       "        [0.0000],\n",
       "        [0.4370],\n",
       "        [0.0000],\n",
       "        [0.0000],\n",
       "        [0.0000],\n",
       "        [0.0182],\n",
       "        [0.0000],\n",
       "        [0.3258],\n",
       "        [0.0000],\n",
       "        [0.0000],\n",
       "        [0.0343],\n",
       "        [0.0000],\n",
       "        [0.0000],\n",
       "        [0.0169],\n",
       "        [0.0000],\n",
       "        [0.1908],\n",
       "        [0.0000],\n",
       "        [0.1288],\n",
       "        [0.0000],\n",
       "        [0.1504],\n",
       "        [0.0000],\n",
       "        [0.0000],\n",
       "        [0.4324],\n",
       "        [0.2295],\n",
       "        [0.0000],\n",
       "        [0.0000],\n",
       "        [0.1320],\n",
       "        [0.0000],\n",
       "        [0.0000],\n",
       "        [0.0000],\n",
       "        [0.2202],\n",
       "        [0.1702],\n",
       "        [0.1251],\n",
       "        [0.0021],\n",
       "        [0.4306],\n",
       "        [0.0000],\n",
       "        [0.0000],\n",
       "        [0.0000],\n",
       "        [0.2210],\n",
       "        [0.0282],\n",
       "        [0.0870],\n",
       "        [0.0000],\n",
       "        [0.0000],\n",
       "        [0.2171]], grad_fn=<ReluBackward0>)"
      ]
     },
     "execution_count": 665,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "critic.critic_net(torch.randn(100, 4, dtype=torch.float))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 668,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[tensor([[0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0.],\n",
       "         ...,\n",
       "         [0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0.]]),\n",
       " tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]]),\n",
       " tensor([0.])]"
      ]
     },
     "execution_count": 668,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[p.grad for p in list(critic.critic_net.parameters())]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-179-11638d783066>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mj\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m         \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ml\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mppo\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m         \u001b[0mfirst\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m         \u001b[0mlast\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ml\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-155-fc42227e1e9d>\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m         \u001b[0mtrajectory_states\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrajectory_actions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0madvantages\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mactor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpolicy\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m         \u001b[0madvantages\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0madvantages\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdetach\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m         \u001b[0mold_probability\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpolicy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprobability\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrajectory_states\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrajectory_actions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdetach\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-24-9b3fd10a9068>\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, policy)\u001b[0m\n\u001b[1;32m     41\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     42\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstate\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat32\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 43\u001b[0;31m             \u001b[0mprevious_state_value\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstate_value\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcritic\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprevious_state\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     44\u001b[0m             \u001b[0mtd_errors\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mreward\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgae_gamma\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mstate_value\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mprevious_state_value\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     45\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-23-12954d57fd1e>\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, previous_state, state, reward)\u001b[0m\n\u001b[1;32m     52\u001b[0m         \u001b[0mtd_error\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mreward\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mstate_value\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mprevious_state_value\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparameter\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcritic_net\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 54\u001b[0;31m             \u001b[0mparameter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlearning_rate\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mtd_error\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meligibility_trace\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     55\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     56\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mprevious_state_value\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstate_value\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for i in range(5):\n",
    "    for j in range(100):\n",
    "        f, l = ppo.step()\n",
    "        first.append(f)\n",
    "        last.append(l)\n",
    "    perf.append(evaluate_policy(env, policy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 782,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make('CartPole-v1')\n",
    "critic_net = CriticNet()\n",
    "tde = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 789,
   "metadata": {},
   "outputs": [],
   "source": [
    "def step():\n",
    "    states = [env.reset()]\n",
    "    rewards = []\n",
    "    state_values = [critic_net(torch.tensor(states[0], dtype=torch.float))]\n",
    "    \n",
    "    while True:\n",
    "        state, reward, done, _ = env.step(env.action_space.sample())\n",
    "        states.append(state)\n",
    "        rewards.append(reward)\n",
    "        state_values.append(critic_net(torch.tensor(state, dtype=torch.float)))\n",
    "        \n",
    "        if done:\n",
    "            break\n",
    "    \n",
    "    for j in range(len(states) - 1):\n",
    "        reward = rewards[j]\n",
    "        previous_state_value = state_values[j]\n",
    "        state_value = state_values[j + 1]\n",
    "            \n",
    "        critic_net.zero_grad()\n",
    "        previous_state_value.backward()\n",
    "        td_error = reward + state_value - previous_state_value\n",
    "            \n",
    "        for p in critic_net.parameters():\n",
    "            p.data += 0.03 * td_error * p.grad.data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 790,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Trying to backward through the graph a second time, but the saved intermediate results have already been freed. Specify retain_graph=True when calling backward the first time.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-790-b283b8c6f0d9>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0m_\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1000\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m     \u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-789-208081387fc0>\u001b[0m in \u001b[0;36mstep\u001b[0;34m()\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m             \u001b[0mcritic_net\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 25\u001b[0;31m             \u001b[0mprevious_state_value\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     26\u001b[0m             \u001b[0mtd_error\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mreward\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mstate_value\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mprevious_state_value\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.8/site-packages/torch/tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph)\u001b[0m\n\u001b[1;32m    183\u001b[0m                 \u001b[0mproducts\u001b[0m\u001b[0;34m.\u001b[0m \u001b[0mDefaults\u001b[0m \u001b[0mto\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    184\u001b[0m         \"\"\"\n\u001b[0;32m--> 185\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    186\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    187\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.8/site-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables)\u001b[0m\n\u001b[1;32m    123\u001b[0m         \u001b[0mretain_graph\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    124\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 125\u001b[0;31m     Variable._execution_engine.run_backward(\n\u001b[0m\u001b[1;32m    126\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    127\u001b[0m         allow_unreachable=True)  # allow_unreachable flag\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Trying to backward through the graph a second time, but the saved intermediate results have already been freed. Specify retain_graph=True when calling backward the first time."
     ]
    }
   ],
   "source": [
    "for _ in range(1000):\n",
    "    step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 780,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def running_mean(x, N):\n",
    "    cumsum = np.cumsum(np.insert(x, 0, 0)) \n",
    "    return (cumsum[N:] - cumsum[:-N]) / float(N)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 781,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7fea90ecf3d0>]"
      ]
     },
     "execution_count": 781,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(running_mean(tde, 100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
