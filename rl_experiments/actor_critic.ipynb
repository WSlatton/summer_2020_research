{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Actor-Critic for CartPole-v1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tile Coding\n",
    "\n",
    "First, we implement basic tile coding to map the continuous state space of the CartPole-v1 task to a feature vector. If the `lower_bounds` parameter consists of $n$ floats $a_0, \\dots, a_{n-1}$, the `upper_bounds` parameter consists of $n$ floats $b_0, \\dots, b_{n-1}$, and the `divisions` parameter consists of $n$ positive integers $d_0, \\dots, d_{n-1}$, then we tile the space $[a_0, b_0) \\times \\cdots \\times [a_{n-1}, b_{n-1})$ uniformly by dividing the $j^\\mathrm{th}$ dimension $[a_j, b_j)$ into $d_j$ equal half-open intervals. Suppose $k_0, ..., k_{n-1}$ are the zero-indexed indices of the intervals in which a point $p$ lies. That is,\n",
    "$$p_j \\in \\bigg[a_j + k_j \\frac{b_j - a_j}{d_j}, a_j + (k_j + 1) \\frac{b_j - a_j}{d_j}\\bigg)$$\n",
    "for each $j$. Then the corresponding feature vector is the $d_0 \\cdots d_{n-1}$-component vector consisting of all zeros except for a one at index\n",
    "$$\\sum_{j=0}^{n-1} (d_{j+1} \\cdots d_{n-1}) k_j.$$\n",
    "\n",
    "Note also that if $p_j < a_j$, then we set $k_j = 0$, and if $p_j \\geq b_j$, then we set $k_j = d_j - 1$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "class TileCoder:\n",
    "    def __init__(self, lower_bounds, upper_bounds, divisions):\n",
    "        self.lower_bounds = lower_bounds\n",
    "        self.upper_bounds = upper_bounds\n",
    "        self.divisions = divisions\n",
    "    \n",
    "    def make_feature_vector(self, point):\n",
    "        n = len(self.lower_bounds)\n",
    "        point = np.clip(point, self.lower_bounds, self.upper_bounds)\n",
    "        k = np.floor((point - self.lower_bounds) * (self.upper_bounds - self.lower_bounds) / self.divisions)\n",
    "        m = round(np.sum([\n",
    "            np.prod([\n",
    "                self.divisions[k]\n",
    "                for k in range(j + 1, n - 1)\n",
    "            ]) * k[j]\n",
    "            for j in range(n - 1)\n",
    "        ]))\n",
    "        x = np.zeros(np.prod(self.divisions))\n",
    "        x[m] = 1\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "tc = TileCoder(\n",
    "    np.array([-4.8, -3, -0.418, -3]),\n",
    "    np.array([+4.8, +3, +0.418, +3]),\n",
    "    np.array([10, 10, 10, 10])\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Actor-Critic with Eligibility Traces"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "action_space = [0, 1]\n",
    "\n",
    "def choose_action(policy, feature_vector, policy_parameters):\n",
    "    random.choices(\n",
    "        action_space,\n",
    "        weights=policy(feature_vector, policy_parameters)\n",
    "    )[0]\n",
    "\n",
    "def actor_critic_episode(\n",
    "    env,\n",
    "    policy, state_value,\n",
    "    policy_parameters, state_value_parameters,\n",
    "    log_policy_grad, state_value_grad,\n",
    "    learning_rate_actor, learning_rate_critic,\n",
    "    trace_decay_actor, trace_decay_critic\n",
    "):\n",
    "    z_actor = np.zeros_like(policy_parameters)\n",
    "    z_critic = np.zeros_like(state_value_parameters)\n",
    "    \n",
    "    state = env.reset()\n",
    "    feature_vector = tc.make_feature_vector(state)\n",
    "    \n",
    "    while True:\n",
    "        action = choose_action(policy, feature_vector, policy_parameters)\n",
    "        next_state, reward, done, _ = env.step(action)\n",
    "        next_feature_vector = tc.make_feature_vector(next_state)\n",
    "        \n",
    "        z_actor = trace_decay_actor * z_actor + log_policy_grad(action, feature_vector, policy_parameters)\n",
    "        z_critic = trace_decay_critic * z_actor + state_value_grad(feature_vector, state_values)\n",
    "        \n",
    "        delta = reward + state_value(next_feature_vector, state_value_parameters) - state_value(feature_vector, state_value_parameters)\n",
    "        \n",
    "        policy_parameters += learning_rate_actor * delta * z_actor\n",
    "        state_value_parameters += learning_rate_critic * delta * z_critic\n",
    "        \n",
    "        state = next_state\n",
    "        feature_vector = next_feature_vector\n",
    "    \n",
    "    return policy_parameters, state_value_parameters\n",
    "\n",
    "def actor_critic(\n",
    "    env,\n",
    "    policy, state_value,\n",
    "    policy_parameters, state_value_parameters,\n",
    "    log_policy_grad, state_value_grad,\n",
    "    learning_rate_actor, learning_rate_critic,\n",
    "    trace_decay_actor, trace_decay_critic,\n",
    "    num_episodes=10\n",
    "):\n",
    "    for episode in range(episodes):\n",
    "        policy_parameters, state_value_parameters = actor_critic_episode(\n",
    "            env,\n",
    "            policy, state_value,\n",
    "            policy_parameters, state_value_parameters,\n",
    "            log_policy_grad, state_value_grad,\n",
    "            learning_rate_actor, learning_rate_critic,\n",
    "            trace_decay_actor, trace_decay_critic\n",
    "        )\n",
    "    \n",
    "    return policy_parameters, state_value_parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def policy(feature_vector, policy_parameters):\n",
    "    pref_matrix = policy_parameters.reshape(len(action_space), -1)\n",
    "    prefs = M.dot(feature_vector)\n",
    "    exps = np.exp(prefs)\n",
    "    softmax = exp / np.sum(exps)\n",
    "    return softmax\n",
    "\n",
    "def log_policy_grad(action, feature_vector, policy_parameters):\n",
    "    pass\n",
    "\n",
    "def state_value(feature_vector, state_value_parameters):\n",
    "    pass\n",
    "\n",
    "def state_value_grad(feature_vector, state_value_parameters):\n",
    "    pass"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
